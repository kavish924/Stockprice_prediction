{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "40HuLSbpIIYm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StockPriceLSTM Model"
      ],
      "metadata": {
        "id": "Q96adjImIDBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockPriceLSTM:\n",
        "    def __init__(self, sequence_length=60, test_size=0.2, validation_size=0.2):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.test_size = test_size\n",
        "        self.validation_size = validation_size\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        df = df.iloc[2:].reset_index(drop=True)\n",
        "\n",
        "        df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n",
        "\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "        numeric_columns = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
        "        for col in numeric_columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        df = df.dropna()\n",
        "\n",
        "        df = df.sort_index()\n",
        "\n",
        "        print(f\"Data loaded successfully!\")\n",
        "        print(f\"Shape: {df.shape}\")\n",
        "        print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
        "        print(f\"Sample data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_features(self, df):\n",
        "        data = df.copy()\n",
        "\n",
        "        data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
        "        data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
        "        data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
        "\n",
        "        delta = data['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        data['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "        data['BB_upper'] = data['MA_20'] + (data['Close'].rolling(window=20).std() * 2)\n",
        "        data['BB_lower'] = data['MA_20'] - (data['Close'].rolling(window=20).std() * 2)\n",
        "\n",
        "        data['Price_Change'] = data['Close'].pct_change()\n",
        "\n",
        "        data['Volume_MA'] = data['Volume'].rolling(window=10).mean()\n",
        "\n",
        "\n",
        "        data['HL_Spread'] = data['High'] - data['Low']\n",
        "\n",
        "        data = data.dropna()\n",
        "\n",
        "        return data\n",
        "\n",
        "    def prepare_sequences(self, data, target_column='Close'):\n",
        "        feature_columns = ['Close', 'High', 'Low', 'Open', 'Volume',\n",
        "                          'MA_5', 'MA_10', 'MA_20', 'RSI', 'Price_Change',\n",
        "                          'Volume_MA', 'HL_Spread']\n",
        "\n",
        "        available_features = [col for col in feature_columns if col in data.columns]\n",
        "\n",
        "        scaled_data = self.scaler.fit_transform(data[available_features])\n",
        "\n",
        "        X, y = [], []\n",
        "        for i in range(self.sequence_length, len(scaled_data)):\n",
        "            X.append(scaled_data[i-self.sequence_length:i])\n",
        "            target_idx = available_features.index(target_column)\n",
        "            y.append(scaled_data[i, target_idx])\n",
        "\n",
        "        return np.array(X), np.array(y), available_features\n",
        "\n",
        "    def split_data(self, X, y):\n",
        "        test_size = int(len(X) * self.test_size)\n",
        "        train_val_size = len(X) - test_size\n",
        "        val_size = int(train_val_size * self.validation_size)\n",
        "        train_size = train_val_size - val_size\n",
        "\n",
        "        X_train = X[:train_size]\n",
        "        X_val = X[train_size:train_size + val_size]\n",
        "        X_test = X[train_size + val_size:]\n",
        "\n",
        "        y_train = y[:train_size]\n",
        "        y_val = y[train_size:train_size + val_size]\n",
        "        y_test = y[train_size + val_size:]\n",
        "\n",
        "        print(f\"Training set size: {len(X_train)}\")\n",
        "        print(f\"Validation set size: {len(X_val)}\")\n",
        "        print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def build_model(self, input_shape):\n",
        "        model = Sequential([\n",
        "            LSTM(units=50, return_sequences=True, input_shape=input_shape),\n",
        "            Dropout(0.2),\n",
        "\n",
        "\n",
        "            LSTM(units=50, return_sequences=True),\n",
        "            Dropout(0.2),\n",
        "\n",
        "\n",
        "            LSTM(units=50, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "\n",
        "\n",
        "            Dense(units=25, activation='relu'),\n",
        "            Dropout(0.1),\n",
        "            Dense(units=1)\n",
        "        ])\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.01),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
        "\n",
        "        self.model = self.build_model((X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=1e-7\n",
        "        )\n",
        "\n",
        "\n",
        "        print(\"Training the model...\")\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    def evaluate_model(self, X_test, y_test):\n",
        "        predictions = self.model.predict(X_test)\n",
        "\n",
        "        mse = mean_squared_error(y_test, predictions)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mape = np.mean(np.abs((y_test - predictions.flatten()) / y_test)) * 100\n",
        "\n",
        "        from sklearn.metrics import r2_score\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "\n",
        "\n",
        "        accuracy_from_mape = 100 - mape\n",
        "\n",
        "        metrics = {\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'RMSE': rmse,\n",
        "            'MAPE (%)': mape,\n",
        "            'R-squared': r2,\n",
        "            'Accuracy (100-MAPE) (%)': accuracy_from_mape\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*30)\n",
        "        print(\"Model Evaluation & Accuracy\")\n",
        "        print(\"=\"*30)\n",
        "        for metric, value in metrics.items():\n",
        "            if metric == 'R-squared':\n",
        "                 print(f\"{metric}: {value:.4f} ({(value * 100):.2f}%)\")\n",
        "            elif '%' in metric:\n",
        "                 print(f\"{metric}: {value:.2f}\")\n",
        "            else:\n",
        "                 print(f\"{metric}: {value:.4f}\")\n",
        "        print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "        return metrics, predictions"
      ],
      "metadata": {
        "id": "aOoTzTGkIJpi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    lstm_model = StockPriceLSTM(sequence_length=60)\n",
        "\n",
        "    df = lstm_model.load_and_preprocess_data('/content/AAPL_stock_data.csv')\n",
        "\n",
        "    df_with_features = lstm_model.create_features(df)\n",
        "\n",
        "    X, y, feature_names = lstm_model.prepare_sequences(df_with_features)\n",
        "    print(f\"Features used: {feature_names}\")\n",
        "\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = lstm_model.split_data(X, y)\n",
        "\n",
        "    lstm_model.train_model(X_train, y_train, X_val, y_val, epochs=50, batch_size=32)\n",
        "\n",
        "\n",
        "    metrics, predictions = lstm_model.evaluate_model(X_test, y_test)\n",
        "    return lstm_model, X, y, feature_names"
      ],
      "metadata": {
        "id": "yqwrL4rShc3N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model, X, y, feature_names = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EluAYUZpiPit",
        "outputId": "a4601336-f137-4249-b857-849301250d17"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "Shape: (2012, 5)\n",
            "Date range: 2016-01-04 00:00:00 to 2023-12-29 00:00:00\n",
            "Sample data:\n",
            "                Close       High        Low       Open     Volume\n",
            "Date                                                             \n",
            "2016-01-04  23.803167  23.807687  23.046256  23.184082  270597600\n",
            "2016-01-05  23.206671  23.916134  23.138889  23.893540  223164000\n",
            "2016-01-06  22.752523  23.129851  22.564992  22.720891  273829600\n",
            "2016-01-07  21.792263  22.623736  21.787745  22.296118  324377600\n",
            "2016-01-08  21.907492  22.393271  21.862304  22.266743  283192000\n",
            "Features used: ['Close', 'High', 'Low', 'Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'Price_Change', 'Volume_MA', 'HL_Spread']\n",
            "Training set size: 1238\n",
            "Validation set size: 309\n",
            "Test set size: 386\n",
            "Training the model...\n",
            "Epoch 1/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 126ms/step - loss: 0.0619 - mae: 0.1528 - val_loss: 0.0371 - val_mae: 0.1830 - learning_rate: 0.0100\n",
            "Epoch 2/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 0.0036 - mae: 0.0396 - val_loss: 0.0035 - val_mae: 0.0453 - learning_rate: 0.0100\n",
            "Epoch 3/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 0.0031 - mae: 0.0380 - val_loss: 0.0228 - val_mae: 0.1395 - learning_rate: 0.0100\n",
            "Epoch 4/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - loss: 0.0015 - mae: 0.0253 - val_loss: 0.0289 - val_mae: 0.1617 - learning_rate: 0.0100\n",
            "Epoch 5/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 0.0018 - mae: 0.0291 - val_loss: 0.0142 - val_mae: 0.1062 - learning_rate: 0.0100\n",
            "Epoch 6/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0195 - val_mae: 0.1282 - learning_rate: 0.0100\n",
            "Epoch 7/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - loss: 0.0017 - mae: 0.0263 - val_loss: 0.0052 - val_mae: 0.0563 - learning_rate: 0.0100\n",
            "Epoch 8/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 0.0179 - val_mae: 0.1201 - learning_rate: 0.0100\n",
            "Epoch 9/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - loss: 0.0016 - mae: 0.0261 - val_loss: 0.0097 - val_mae: 0.0813 - learning_rate: 0.0100\n",
            "Epoch 10/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - loss: 0.0015 - mae: 0.0282 - val_loss: 0.0060 - val_mae: 0.0621 - learning_rate: 0.0100\n",
            "Epoch 11/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0103 - val_mae: 0.0866 - learning_rate: 0.0100\n",
            "Epoch 12/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0129 - val_mae: 0.1006 - learning_rate: 0.0100\n",
            "Epoch 13/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0032 - val_mae: 0.0426 - learning_rate: 0.0050\n",
            "Epoch 14/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 9.8289e-04 - mae: 0.0216 - val_loss: 0.0080 - val_mae: 0.0751 - learning_rate: 0.0050\n",
            "Epoch 15/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 8.6748e-04 - mae: 0.0201 - val_loss: 0.0069 - val_mae: 0.0685 - learning_rate: 0.0050\n",
            "Epoch 16/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - loss: 7.5890e-04 - mae: 0.0184 - val_loss: 0.0055 - val_mae: 0.0591 - learning_rate: 0.0050\n",
            "Epoch 17/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 9.5935e-04 - mae: 0.0193 - val_loss: 0.0069 - val_mae: 0.0700 - learning_rate: 0.0050\n",
            "Epoch 18/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 9.4005e-04 - mae: 0.0211 - val_loss: 0.0087 - val_mae: 0.0780 - learning_rate: 0.0050\n",
            "Epoch 19/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - loss: 7.9624e-04 - mae: 0.0187 - val_loss: 0.0072 - val_mae: 0.0694 - learning_rate: 0.0050\n",
            "Epoch 20/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 7.5022e-04 - mae: 0.0187 - val_loss: 0.0083 - val_mae: 0.0764 - learning_rate: 0.0050\n",
            "Epoch 21/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 8.1344e-04 - mae: 0.0195 - val_loss: 0.0130 - val_mae: 0.1013 - learning_rate: 0.0050\n",
            "Epoch 22/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - loss: 0.0011 - mae: 0.0226 - val_loss: 0.0085 - val_mae: 0.0755 - learning_rate: 0.0050\n",
            "Epoch 23/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 8.7559e-04 - mae: 0.0209 - val_loss: 0.0045 - val_mae: 0.0518 - learning_rate: 0.0050\n",
            "Epoch 24/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 8.4050e-04 - mae: 0.0183 - val_loss: 0.0131 - val_mae: 0.1029 - learning_rate: 0.0025\n",
            "Epoch 25/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - loss: 8.5675e-04 - mae: 0.0196 - val_loss: 0.0100 - val_mae: 0.0863 - learning_rate: 0.0025\n",
            "Epoch 26/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 7.8471e-04 - mae: 0.0181 - val_loss: 0.0091 - val_mae: 0.0810 - learning_rate: 0.0025\n",
            "Epoch 27/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 8.9139e-04 - mae: 0.0197 - val_loss: 0.0101 - val_mae: 0.0871 - learning_rate: 0.0025\n",
            "Epoch 28/50\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 7.2648e-04 - mae: 0.0181 - val_loss: 0.0093 - val_mae: 0.0833 - learning_rate: 0.0025\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step\n",
            "\n",
            "==============================\n",
            "Model Evaluation & Accuracy\n",
            "==============================\n",
            "MSE: 0.0096\n",
            "MAE: 0.0787\n",
            "RMSE: 0.0977\n",
            "MAPE (%): 9.06\n",
            "R-squared: 0.1868 (18.68%)\n",
            "Accuracy (100-MAPE) (%): 90.94\n",
            "==============================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}